# -*- coding: utf-8 -*-
"""end to end project(used car prediction).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WPoAJmEJI4lP-4Sm5ygvIjbmttulRt2q
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn
import seaborn as sns

df=pd.read_csv("/content/car data.csv")
df

df.isnull().sum()

df.describe().transpose()

df.shape

df.columns

df['Seller_Type'].unique()

df['Fuel_Type'].unique()

"""# feature engineering:"""

df['current_year']=2022
df

"""to find the age of the car:"""

df["car_age"]=df['current_year']-df["Year"]
df

"""# by dummy variables:"""

# finaldf=pd.get_dummies(data=df)
# finaldf

# finaldf.corr()

"""plotting of heatmap to find correlation:"""

# corr=finaldf.corr()
# sns.heatmap(finaldf,annot=True)

"""separation of x and y axis:"""

x=df.drop(['Selling_Price','Present_Price'],axis=1)
x

"""# *by* label encoder:"""

from sklearn.preprocessing import LabelEncoder

le=LabelEncoder()

for col in x:
  x[col]=le.fit(x[col]).transform(x[col])

x

y=df['Selling_Price']
y

# x=finaldf.iloc[:,:] # all rows and columns

# y=finaldf.iloc[:,1] # all rows and first colmns which starts from zero

"""importing **model_selection**"""

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.5,random_state=1)

"""# random forest regressor: it is an algorithm which is similar to the decision tree and this is the enhanced version of the decision tree. it is like forming forest of trees."""

from sklearn.ensemble import RandomForestRegressor

regression=RandomForestRegressor()

model=regression.fit(xtrain,ytrain)

model

model.predict(xtrain)

from sklearn.metrics import mean_squared_error,r2_score
xpred=model.predict(xtest)
ac=mean_squared_error(xpred,ytest)

r2ss=r2_score(xpred,ytest)

print(ac)
print(r2ss)
# here above we got an accuracy by means of using labelencoder i can also use dummy variable method from above by changing the variable name.

model.predict([[90,11,86,2,0,1,0,0,4]])# here we have a slight error in prediction of our model we get 5.7305 instead of 3.35

"""# feature importance:

it is a method through which we can find the necessary or releavent feature or input x. for the given dataset
"""

from sklearn.ensemble import ExtraTreesRegressor
model=ExtraTreesRegressor()

model.fit(x,y)

print(model.feature_importances_)

feature_importance=pd.Series(model.feature_importances_,index=x.columns)

feature_importance.nlargest(8).plot(kind='barh')

"""# linear regression"""

from sklearn.linear_model import LinearRegression

lr=LinearRegression()

lr.fit(xtrain,ytrain)

xtrain.shape,ytrain.shape

lr.predict(xtrain)

lr.predict([[90,11,86,2,0,1,0,0,4]])

from sklearn.metrics import mean_squared_error,r2_score
xpred=lr.predict(xtest)
mse=mean_squared_error(xpred,ytest)
r2s=r2_score(xpred,ytest)

print(mse)
print(r2s)

"""# plotting of regression line:"""

plt.scatter(xtrain,ytrain,label="actual",marker='*',color="red")
plt.plot(xtest,lr.predict(xtest),label="predicted")
plt.legend()
plt.show()

"""# decision tree regressor:"""

from sklearn.tree import DecisionTreeRegressor

dtr=DecisionTreeRegressor()

modeldtr=dtr.fit(xtrain,ytrain)

modeldtr.predict(xtrain)

modeldtr.predict([[90,11,86,2,0,1,0,0,9]])

from sklearn.metrics import mean_squared_error,r2_score
xpred=dtr.predict(xtest)
mse1=mean_squared_error(xpred,ytest)
r2=r2_score(xpred,ytest)

print("the meansquared error is:",mse1)
print("the r2_score of model is:",r2)

"""# hyperparameter tuning:

it is the process through which we can select optimal algorithm or best thing that helps with the improvement the accuracy of the model.

1.kfold cross validation- which helps in the verification or choosing the best algorithm which gives high accuracy. 

other methods available are:

1.randomized search cv

2.grid search cv


--> applying hyper parameter tuning for decision tree algorithm:
"""

n_estimators=[int(x) for x in np.linspace(start=100,stop=1200,num=12)] # linspace function returns the evenly spaced number.

print(n_estimators)

from sklearn.model_selection import RandomizedSearchCV

n_estimators=[int(x) for x in np.linspace(start=100,stop=1200,num=12)] # this line is used for number of trees in random forest

# number of features to consider at every split:

max_features=['auto','sqrt']

# maximum no of levels in tree:

max_depth=[int(x) for x in np.linspace(5,30,num=6)]

# max_depth.append(None)

# Minimum number of samples required to split a node

min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node

min_samples_leaf = [1, 2, 5, 10]

# to create a random grid: and storing it inside dictionary.

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)

# Use the random grid to search for best hyperparameters
# First create the base model to tune
rf = RandomForestRegressor()

# Random search of parameters, using 3 fold cross validation, 
# search across 100 different combinations
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 100, cv = 5, verbose=2)

rf_random.fit(xtrain,ytrain)

rf_random.best_params_

rf_random.best_score_

finalresult=rf_random.predict(xtest)
finalresult

from sklearn.metrics import mean_squared_error,r2_score
mse11=mean_squared_error(finalresult,ytest)

r2sss=r2_score(finalresult,ytest)

print(mse11)

print(r2sss)

import pickle

pickle.dump(model,open('carprediction.pkl','wb'))

